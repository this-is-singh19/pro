{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/this-is-singh19/tbdetectx/blob/master/ssdresnet50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JTnH9kCFOsjq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import EfficientNetB0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqUSj7zfmjT8",
        "outputId": "abbd5f3f-7266-4e41-a1ab-35497b2a971c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "dataset_path = '/content/drive/My Drive/Dataset/imgs'\n",
        "os.chdir(dataset_path)"
      ],
      "metadata": {
        "id": "g4oxYzWvmt_t"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dirlist = ['../imgs/health/', '../imgs/sick/', '../imgs/tb']\n",
        "classes = ['Healthy', 'Sick', 'Tuberculosis']\n",
        "filepaths = []\n",
        "labels = []\n",
        "for d, c in zip(dirlist, classes):\n",
        "    flist = os.listdir(d)\n",
        "    for f in flist:\n",
        "        fpath = os.path.join(d, f)\n",
        "        filepaths.append(fpath)\n",
        "        labels.append(c)\n",
        "print ('filepaths: ', len(filepaths), '   labels: ', len(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jP_dXEWBmv99",
        "outputId": "da856f10-91c7-4044-f503-e2df773687e9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "filepaths:  8408    labels:  8408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Fseries = pd.Series(filepaths, name='file_paths')\n",
        "Lseries = pd.Series(labels, name='labels')\n",
        "\n",
        "# Ensure lengths match\n",
        "min_length = min(len(Fseries), len(Lseries))\n",
        "Fseries = Fseries[:min_length]\n",
        "Lseries = Lseries[:min_length]\n",
        "\n",
        "# Create the DataFrame with named columns\n",
        "df = pd.concat([Fseries, Lseries], axis=1)\n",
        "df.columns = ['file_paths', 'labels']\n",
        "\n",
        "# Count occurrences of each label\n",
        "label_counts = df['labels'].value_counts()\n",
        "print(label_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-T5Djy6mybb",
        "outputId": "868a1836-bbf3-4bd8-ad23-d4b6666e836c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Healthy         3814\n",
            "Sick            3809\n",
            "Tuberculosis     785\n",
            "Name: labels, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_count = 1500\n",
        "samples = []\n",
        "\n",
        "for category in df['labels'].unique():\n",
        "    category_slice = df.query(\"labels == @category\")\n",
        "\n",
        "    if len(category_slice) < file_count:\n",
        "        # If the number of files in the category is less than file_count,\n",
        "        # sample with replacement to fill up the required number of samples\n",
        "        samples.append(category_slice.sample(file_count, replace=True, random_state=1))\n",
        "    else:\n",
        "        samples.append(category_slice.sample(file_count, replace=False, random_state=1))\n",
        "\n",
        "df = pd.concat(samples, axis=0).sample(frac=1.0, random_state=1).reset_index(drop=True)\n",
        "print(df['labels'].value_counts())\n",
        "print(len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWCFLCTnm0tV",
        "outputId": "4be9fe1c-9da3-439e-9ccb-d766ec5d7dba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sick            1500\n",
            "Healthy         1500\n",
            "Tuberculosis    1500\n",
            "Name: labels, dtype: int64\n",
            "4500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(df, train_size=0.7, valid_size=0.15, test_size=0.15, random_state=None):\n",
        "    \"\"\"\n",
        "    Split the data into training, validation, and test sets.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame containing the data to be split.\n",
        "    - train_size: The proportion of data to include in the training set (default: 0.7).\n",
        "    - valid_size: The proportion of data to include in the validation set (default: 0.15).\n",
        "    - test_size: The proportion of data to include in the test set (default: 0.15).\n",
        "    - random_state: Seed for random number generation (optional).\n",
        "\n",
        "    Returns:\n",
        "    - train_df: DataFrame for training.\n",
        "    - valid_df: DataFrame for validation.\n",
        "    - test_df: DataFrame for testing.\n",
        "    \"\"\"\n",
        "    if train_size + valid_size + test_size != 1.0:\n",
        "        raise ValueError(\"The sum of train_size, valid_size, and test_size should be 1.0\")\n",
        "\n",
        "    # Split the data into training and test sets\n",
        "    train_and_valid_df, test_df = train_test_split(df, test_size=test_size, random_state=random_state)\n",
        "\n",
        "    # Further split the training and validation data\n",
        "    train_df, valid_df = train_test_split(train_and_valid_df,\n",
        "                                          train_size=train_size / (train_size + valid_size),\n",
        "                                          random_state=random_state)\n",
        "\n",
        "    return train_df, valid_df, test_df\n",
        "\n",
        "def print_label_counts(df, set_name):\n",
        "    \"\"\"\n",
        "    Print label counts for a given DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame for which label counts should be printed.\n",
        "    - set_name: Name of the data set (e.g., \"Training\", \"Validation\", \"Test\").\n",
        "    \"\"\"\n",
        "    print(f\"{set_name} Set Label Counts:\")\n",
        "    label_counts = df['labels'].value_counts()\n",
        "    print(label_counts)\n",
        "\n",
        "# Split the data into train, validation, and test sets\n",
        "train_df, valid_df, test_df = split_data(df, train_size=0.7, valid_size=0.15, test_size=0.15, random_state=0)\n",
        "\n",
        "# Print label counts for each set\n",
        "print_label_counts(train_df, \"Training\")\n",
        "print_label_counts(valid_df, \"Validation\")\n",
        "print_label_counts(test_df, \"Test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pC9MI_UVm3n9",
        "outputId": "0e96a919-240e-47ec-c77e-633d7f9fe89f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Label Counts:\n",
            "Sick            1066\n",
            "Healthy         1044\n",
            "Tuberculosis    1040\n",
            "Name: labels, dtype: int64\n",
            "Validation Set Label Counts:\n",
            "Healthy         233\n",
            "Tuberculosis    229\n",
            "Sick            213\n",
            "Name: labels, dtype: int64\n",
            "Test Set Label Counts:\n",
            "Tuberculosis    231\n",
            "Healthy         223\n",
            "Sick            221\n",
            "Name: labels, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "# Define the input shape\n",
        "input_shape = (224, 224, 3)\n",
        "\n",
        "# Create the ResNet50 base model\n",
        "resnet50 = ResNet50(input_shape=input_shape, include_top=False, weights='imagenet')\n",
        "\n",
        "# Allow some layers of ResNet50 to be trainable\n",
        "for layer in resnet50.layers[:-10]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add SSD layers on top of the base model\n",
        "ssd_model = models.Sequential()\n",
        "ssd_model.add(resnet50)\n",
        "ssd_model.add(layers.Conv2D(512, (3, 3), padding='same'))\n",
        "ssd_model.add(layers.BatchNormalization())\n",
        "ssd_model.add(layers.Activation('relu'))\n",
        "ssd_model.add(layers.MaxPooling2D((2, 2), strides=2))\n",
        "ssd_model.add(layers.Conv2D(1024, (3, 3), padding='same'))\n",
        "ssd_model.add(layers.Conv2D(2048, (3, 3), padding='same'))\n",
        "ssd_model.add(layers.Flatten())\n",
        "ssd_model.add(layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
        "ssd_model.add(layers.BatchNormalization())\n",
        "ssd_model.add(layers.Dropout(0.5))\n",
        "ssd_model.add(layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
        "ssd_model.add(layers.BatchNormalization())\n",
        "ssd_model.add(layers.Dropout(0.5))\n",
        "ssd_model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "# Compile the SSD model with a learning rate scheduler\n",
        "initial_learning_rate = 0.0001\n",
        "lr_scheduler = ReduceLROnPlateau(factor=0.1, patience=3, min_lr=1e-7)\n",
        "\n",
        "ssd_model.compile(optimizer=Adam(learning_rate=initial_learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define the data generators for training, validation, and test with increased data augmentation\n",
        "target_size = (224, 224)\n",
        "batch_size = 4\n",
        "\n",
        "train_datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.efficientnet.preprocess_input, horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.efficientnet.preprocess_input)\n",
        "train_gen = train_datagen.flow_from_dataframe(train_df, x_col='file_paths', y_col='labels', target_size=target_size, batch_size=batch_size, color_mode='rgb', class_mode='categorical')\n",
        "valid_gen = test_datagen.flow_from_dataframe(valid_df, x_col='file_paths', y_col='labels', target_size=target_size, batch_size=batch_size, color_mode='rgb', class_mode='categorical')\n",
        "test_gen = test_datagen.flow_from_dataframe(test_df, x_col='file_paths', y_col='labels', target_size=target_size, batch_size=batch_size, color_mode='rgb', class_mode='categorical')\n",
        "\n",
        "\n",
        "# Train the SSD model with learning rate scheduler\n",
        "ssd_model.fit(train_generator, epochs=50, validation_data=valid_generator, callbacks=[lr_scheduler])\n",
        "\n",
        "# Evaluate the SSD model on the test set\n",
        "test_loss, test_accuracy = ssd_model.evaluate(test_generator)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "# Save the SSD model\n",
        "ssd_model.save('ssd_resnet50.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_t6txVYiWN2",
        "outputId": "2bfa90a5-d773-4824-c853-f19e3a24d96f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3150 validated image filenames belonging to 3 classes.\n",
            "Found 675 validated image filenames belonging to 3 classes.\n",
            "Found 675 validated image filenames belonging to 3 classes.\n",
            "Epoch 1/50\n",
            "99/99 [==============================] - 326s 3s/step - loss: 11.6732 - accuracy: 0.6238 - val_loss: 18.1684 - val_accuracy: 0.3452 - lr: 1.0000e-04\n",
            "Epoch 2/50\n",
            "99/99 [==============================] - 81s 818ms/step - loss: 9.7748 - accuracy: 0.7073 - val_loss: 20.0844 - val_accuracy: 0.3156 - lr: 1.0000e-04\n",
            "Epoch 3/50\n",
            "99/99 [==============================] - 79s 804ms/step - loss: 8.7049 - accuracy: 0.7038 - val_loss: 9.9859 - val_accuracy: 0.6133 - lr: 1.0000e-04\n",
            "Epoch 4/50\n",
            "99/99 [==============================] - 79s 801ms/step - loss: 7.7966 - accuracy: 0.7089 - val_loss: 17.5295 - val_accuracy: 0.3156 - lr: 1.0000e-04\n",
            "Epoch 5/50\n",
            "99/99 [==============================] - 80s 808ms/step - loss: 7.0164 - accuracy: 0.7352 - val_loss: 73.6979 - val_accuracy: 0.3156 - lr: 1.0000e-04\n",
            "Epoch 6/50\n",
            "99/99 [==============================] - 79s 802ms/step - loss: 6.4117 - accuracy: 0.7292 - val_loss: 51.4651 - val_accuracy: 0.3156 - lr: 1.0000e-04\n",
            "Epoch 7/50\n",
            "99/99 [==============================] - 82s 821ms/step - loss: 5.9602 - accuracy: 0.7597 - val_loss: 10.5032 - val_accuracy: 0.3156 - lr: 1.0000e-05\n",
            "Epoch 8/50\n",
            "99/99 [==============================] - 79s 799ms/step - loss: 5.8891 - accuracy: 0.7641 - val_loss: 5.7198 - val_accuracy: 0.8148 - lr: 1.0000e-05\n",
            "Epoch 9/50\n",
            "99/99 [==============================] - 81s 821ms/step - loss: 5.8386 - accuracy: 0.7759 - val_loss: 5.8402 - val_accuracy: 0.7733 - lr: 1.0000e-05\n",
            "Epoch 10/50\n",
            "99/99 [==============================] - 82s 833ms/step - loss: 5.7306 - accuracy: 0.7813 - val_loss: 5.5100 - val_accuracy: 0.8237 - lr: 1.0000e-05\n",
            "Epoch 11/50\n",
            "99/99 [==============================] - 79s 798ms/step - loss: 5.6366 - accuracy: 0.7987 - val_loss: 5.9010 - val_accuracy: 0.7585 - lr: 1.0000e-05\n",
            "Epoch 12/50\n",
            "99/99 [==============================] - 80s 809ms/step - loss: 5.5368 - accuracy: 0.8006 - val_loss: 5.3105 - val_accuracy: 0.8533 - lr: 1.0000e-05\n",
            "Epoch 13/50\n",
            "99/99 [==============================] - 81s 817ms/step - loss: 5.4657 - accuracy: 0.7952 - val_loss: 5.8463 - val_accuracy: 0.6770 - lr: 1.0000e-05\n",
            "Epoch 14/50\n",
            "99/99 [==============================] - 79s 800ms/step - loss: 5.3535 - accuracy: 0.7994 - val_loss: 5.3874 - val_accuracy: 0.8178 - lr: 1.0000e-05\n",
            "Epoch 15/50\n",
            "99/99 [==============================] - 80s 803ms/step - loss: 5.2705 - accuracy: 0.8060 - val_loss: 5.8330 - val_accuracy: 0.7615 - lr: 1.0000e-05\n",
            "Epoch 16/50\n",
            "99/99 [==============================] - 80s 807ms/step - loss: 5.2065 - accuracy: 0.8013 - val_loss: 5.0764 - val_accuracy: 0.8385 - lr: 1.0000e-06\n",
            "Epoch 17/50\n",
            "99/99 [==============================] - 79s 798ms/step - loss: 5.2043 - accuracy: 0.8000 - val_loss: 4.9704 - val_accuracy: 0.8726 - lr: 1.0000e-06\n",
            "Epoch 18/50\n",
            "99/99 [==============================] - 81s 810ms/step - loss: 5.1728 - accuracy: 0.8092 - val_loss: 4.9463 - val_accuracy: 0.8770 - lr: 1.0000e-06\n",
            "Epoch 19/50\n",
            "99/99 [==============================] - 78s 785ms/step - loss: 5.2023 - accuracy: 0.8000 - val_loss: 4.9656 - val_accuracy: 0.8607 - lr: 1.0000e-06\n",
            "Epoch 20/50\n",
            "99/99 [==============================] - 80s 804ms/step - loss: 5.1452 - accuracy: 0.8111 - val_loss: 4.9418 - val_accuracy: 0.8711 - lr: 1.0000e-06\n",
            "Epoch 21/50\n",
            "99/99 [==============================] - 79s 794ms/step - loss: 5.1551 - accuracy: 0.8057 - val_loss: 4.9052 - val_accuracy: 0.8904 - lr: 1.0000e-06\n",
            "Epoch 22/50\n",
            "99/99 [==============================] - 80s 802ms/step - loss: 5.1142 - accuracy: 0.8124 - val_loss: 4.8989 - val_accuracy: 0.8756 - lr: 1.0000e-06\n",
            "Epoch 23/50\n",
            "99/99 [==============================] - 79s 801ms/step - loss: 5.1143 - accuracy: 0.8111 - val_loss: 4.8725 - val_accuracy: 0.8919 - lr: 1.0000e-06\n",
            "Epoch 24/50\n",
            "99/99 [==============================] - 81s 818ms/step - loss: 5.0911 - accuracy: 0.8102 - val_loss: 4.8593 - val_accuracy: 0.8859 - lr: 1.0000e-06\n",
            "Epoch 25/50\n",
            "99/99 [==============================] - 81s 815ms/step - loss: 5.0825 - accuracy: 0.8203 - val_loss: 4.8527 - val_accuracy: 0.8830 - lr: 1.0000e-06\n",
            "Epoch 26/50\n",
            "99/99 [==============================] - 81s 815ms/step - loss: 5.0472 - accuracy: 0.8241 - val_loss: 4.8191 - val_accuracy: 0.8993 - lr: 1.0000e-06\n",
            "Epoch 27/50\n",
            "99/99 [==============================] - 82s 833ms/step - loss: 5.0719 - accuracy: 0.8089 - val_loss: 4.8139 - val_accuracy: 0.8978 - lr: 1.0000e-06\n",
            "Epoch 28/50\n",
            "99/99 [==============================] - 80s 806ms/step - loss: 5.0383 - accuracy: 0.8086 - val_loss: 4.8192 - val_accuracy: 0.8859 - lr: 1.0000e-06\n",
            "Epoch 29/50\n",
            "99/99 [==============================] - 82s 828ms/step - loss: 5.0376 - accuracy: 0.8114 - val_loss: 4.8270 - val_accuracy: 0.8741 - lr: 1.0000e-06\n",
            "Epoch 30/50\n",
            "99/99 [==============================] - 80s 804ms/step - loss: 5.0051 - accuracy: 0.8146 - val_loss: 4.7844 - val_accuracy: 0.8815 - lr: 1.0000e-06\n",
            "Epoch 31/50\n",
            "99/99 [==============================] - 78s 787ms/step - loss: 4.9804 - accuracy: 0.8225 - val_loss: 4.7607 - val_accuracy: 0.8919 - lr: 1.0000e-06\n",
            "Epoch 32/50\n",
            "99/99 [==============================] - 81s 814ms/step - loss: 4.9518 - accuracy: 0.8251 - val_loss: 4.7455 - val_accuracy: 0.8874 - lr: 1.0000e-06\n",
            "Epoch 33/50\n",
            "99/99 [==============================] - 78s 788ms/step - loss: 4.9282 - accuracy: 0.8248 - val_loss: 4.7109 - val_accuracy: 0.9052 - lr: 1.0000e-06\n",
            "Epoch 34/50\n",
            "99/99 [==============================] - 80s 802ms/step - loss: 4.9140 - accuracy: 0.8149 - val_loss: 4.6994 - val_accuracy: 0.9022 - lr: 1.0000e-06\n",
            "Epoch 35/50\n",
            "99/99 [==============================] - 78s 785ms/step - loss: 4.8935 - accuracy: 0.8206 - val_loss: 4.6772 - val_accuracy: 0.8963 - lr: 1.0000e-06\n",
            "Epoch 36/50\n",
            "99/99 [==============================] - 79s 802ms/step - loss: 4.8715 - accuracy: 0.8257 - val_loss: 4.6473 - val_accuracy: 0.9007 - lr: 1.0000e-06\n",
            "Epoch 37/50\n",
            "99/99 [==============================] - 79s 800ms/step - loss: 4.8793 - accuracy: 0.8225 - val_loss: 4.6370 - val_accuracy: 0.8904 - lr: 1.0000e-06\n",
            "Epoch 38/50\n",
            "99/99 [==============================] - 79s 800ms/step - loss: 4.8463 - accuracy: 0.8133 - val_loss: 4.6075 - val_accuracy: 0.8978 - lr: 1.0000e-06\n",
            "Epoch 39/50\n",
            "99/99 [==============================] - 80s 813ms/step - loss: 4.8077 - accuracy: 0.8279 - val_loss: 4.6020 - val_accuracy: 0.8963 - lr: 1.0000e-06\n",
            "Epoch 40/50\n",
            "99/99 [==============================] - 80s 808ms/step - loss: 4.7983 - accuracy: 0.8181 - val_loss: 4.5977 - val_accuracy: 0.8859 - lr: 1.0000e-06\n",
            "Epoch 41/50\n",
            "99/99 [==============================] - 84s 849ms/step - loss: 4.7532 - accuracy: 0.8175 - val_loss: 4.6222 - val_accuracy: 0.8696 - lr: 1.0000e-06\n",
            "Epoch 42/50\n",
            "99/99 [==============================] - 80s 807ms/step - loss: 4.7340 - accuracy: 0.8260 - val_loss: 4.5455 - val_accuracy: 0.8919 - lr: 1.0000e-06\n",
            "Epoch 43/50\n",
            "99/99 [==============================] - 81s 813ms/step - loss: 4.7230 - accuracy: 0.8168 - val_loss: 4.5093 - val_accuracy: 0.8948 - lr: 1.0000e-06\n",
            "Epoch 44/50\n",
            "99/99 [==============================] - 80s 806ms/step - loss: 4.7047 - accuracy: 0.8178 - val_loss: 4.4946 - val_accuracy: 0.8948 - lr: 1.0000e-06\n",
            "Epoch 45/50\n",
            "99/99 [==============================] - 80s 808ms/step - loss: 4.6957 - accuracy: 0.8102 - val_loss: 4.4498 - val_accuracy: 0.8948 - lr: 1.0000e-06\n",
            "Epoch 46/50\n",
            "99/99 [==============================] - 79s 797ms/step - loss: 4.6180 - accuracy: 0.8381 - val_loss: 4.4431 - val_accuracy: 0.8859 - lr: 1.0000e-06\n",
            "Epoch 47/50\n",
            "99/99 [==============================] - 83s 843ms/step - loss: 4.6148 - accuracy: 0.8308 - val_loss: 4.3813 - val_accuracy: 0.9067 - lr: 1.0000e-06\n",
            "Epoch 48/50\n",
            "99/99 [==============================] - 80s 808ms/step - loss: 4.6055 - accuracy: 0.8216 - val_loss: 4.3882 - val_accuracy: 0.8948 - lr: 1.0000e-06\n",
            "Epoch 49/50\n",
            "99/99 [==============================] - 78s 788ms/step - loss: 4.5935 - accuracy: 0.8146 - val_loss: 4.3364 - val_accuracy: 0.9096 - lr: 1.0000e-06\n",
            "Epoch 50/50\n",
            "99/99 [==============================] - 84s 845ms/step - loss: 4.5135 - accuracy: 0.8349 - val_loss: 4.3218 - val_accuracy: 0.9096 - lr: 1.0000e-06\n",
            "22/22 [==============================] - 224s 11s/step - loss: 4.2915 - accuracy: 0.9037\n",
            "Test Accuracy: 90.37%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score, recall_score\n",
        "\n",
        "\n",
        "y_pred_prob = ssd_model.predict(test_generator)\n",
        "\n",
        "# Calculate accuracy\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "# Calculate AUC\n",
        "auc = roc_auc_score(y_true, y_pred_prob, multi_class='ovr', average='weighted')\n",
        "print(\"AUC:\", auc)\n",
        "\n",
        "# Calculate average precision\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Binarize the true labels\n",
        "y_true_bin = label_binarize(y_true, classes=['Tuberculosis'])  # Replace [...] with your class labels\n",
        "\n",
        "# Calculate average precision for each class\n",
        "average_precision = []\n",
        "for i in range(y_true_bin.shape[1]):\n",
        "    class_average_precision = average_precision_score(y_true_bin[:, i], y_pred_prob[:, i])\n",
        "    average_precision.append(class_average_precision)\n",
        "\n",
        "# Calculate the weighted average of class average precisions\n",
        "weighted_average_precision = np.average(average_precision)\n",
        "\n",
        "print(\"Weighted Average Precision:\", weighted_average_precision)\n",
        "\n",
        "# Calculate average recall\n",
        "average_recall = recall_score(y_true, y_pred, average='weighted')\n",
        "print(\"Average Recall:\", average_recall)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcdAduj4Kfdt",
        "outputId": "06ca8580-4af9-44d7-f887-2a1539cd9df9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 8s 348ms/step\n",
            "AUC: 0.5060577596426101\n",
            "Weighted Average Precision: 0.0\n",
            "Average Recall: 0.3422222222222222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/arraysetops.py:612: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPK2z23bqk7zmEQGTh88ZmB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}