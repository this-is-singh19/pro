{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0gO/c1zIQldg7X/14Hwan",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/this-is-singh19/tbdetectx/blob/master/ssd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-CPpgeQGI7Dz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import EfficientNetB0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WAPlEx1Ytlp",
        "outputId": "390e9953-6cdd-4f80-b31c-acd0b3e4f3a3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "dataset_path = '/content/drive/My Drive/Dataset/imgs'\n",
        "os.chdir(dataset_path)"
      ],
      "metadata": {
        "id": "4QWXzYm5YuSJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dirlist = ['../imgs/health/', '../imgs/sick/', '../imgs/tb']\n",
        "classes = ['Healthy', 'Sick', 'Tuberculosis']\n",
        "filepaths = []\n",
        "labels = []\n",
        "for d, c in zip(dirlist, classes):\n",
        "    flist = os.listdir(d)\n",
        "    for f in flist:\n",
        "        fpath = os.path.join(d, f)\n",
        "        filepaths.append(fpath)\n",
        "        labels.append(c)\n",
        "print ('filepaths: ', len(filepaths), '   labels: ', len(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTzttUJtY4w5",
        "outputId": "ec47564e-75b2-42fa-fb53-97b8c3ca7697"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "filepaths:  8408    labels:  8408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Fseries = pd.Series(filepaths, name='file_paths')\n",
        "Lseries = pd.Series(labels, name='labels')\n",
        "\n",
        "# Ensure lengths match\n",
        "min_length = min(len(Fseries), len(Lseries))\n",
        "Fseries = Fseries[:min_length]\n",
        "Lseries = Lseries[:min_length]\n",
        "\n",
        "# Create the DataFrame with named columns\n",
        "df = pd.concat([Fseries, Lseries], axis=1)\n",
        "df.columns = ['file_paths', 'labels']\n",
        "\n",
        "# Count occurrences of each label\n",
        "label_counts = df['labels'].value_counts()\n",
        "print(label_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFklkjFSY6sY",
        "outputId": "402b6d74-fefd-46ce-dc01-c81b9e93a5c9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Healthy         3814\n",
            "Sick            3809\n",
            "Tuberculosis     785\n",
            "Name: labels, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_count = 1500\n",
        "samples = []\n",
        "\n",
        "for category in df['labels'].unique():\n",
        "    category_slice = df.query(\"labels == @category\")\n",
        "\n",
        "    if len(category_slice) < file_count:\n",
        "        # If the number of files in the category is less than file_count,\n",
        "        # sample with replacement to fill up the required number of samples\n",
        "        samples.append(category_slice.sample(file_count, replace=True, random_state=1))\n",
        "    else:\n",
        "        samples.append(category_slice.sample(file_count, replace=False, random_state=1))\n",
        "\n",
        "df = pd.concat(samples, axis=0).sample(frac=1.0, random_state=1).reset_index(drop=True)\n",
        "print(df['labels'].value_counts())\n",
        "print(len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxJbxqaEY-gJ",
        "outputId": "fc2637ab-caeb-4d1d-b313-a303b21b642f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sick            1500\n",
            "Healthy         1500\n",
            "Tuberculosis    1500\n",
            "Name: labels, dtype: int64\n",
            "4500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(df, train_size=0.7, valid_size=0.15, test_size=0.15, random_state=None):\n",
        "    \"\"\"\n",
        "    Split the data into training, validation, and test sets.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame containing the data to be split.\n",
        "    - train_size: The proportion of data to include in the training set (default: 0.7).\n",
        "    - valid_size: The proportion of data to include in the validation set (default: 0.15).\n",
        "    - test_size: The proportion of data to include in the test set (default: 0.15).\n",
        "    - random_state: Seed for random number generation (optional).\n",
        "\n",
        "    Returns:\n",
        "    - train_df: DataFrame for training.\n",
        "    - valid_df: DataFrame for validation.\n",
        "    - test_df: DataFrame for testing.\n",
        "    \"\"\"\n",
        "    if train_size + valid_size + test_size != 1.0:\n",
        "        raise ValueError(\"The sum of train_size, valid_size, and test_size should be 1.0\")\n",
        "\n",
        "    # Split the data into training and test sets\n",
        "    train_and_valid_df, test_df = train_test_split(df, test_size=test_size, random_state=random_state)\n",
        "\n",
        "    # Further split the training and validation data\n",
        "    train_df, valid_df = train_test_split(train_and_valid_df,\n",
        "                                          train_size=train_size / (train_size + valid_size),\n",
        "                                          random_state=random_state)\n",
        "\n",
        "    return train_df, valid_df, test_df\n",
        "\n",
        "def print_label_counts(df, set_name):\n",
        "    \"\"\"\n",
        "    Print label counts for a given DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame for which label counts should be printed.\n",
        "    - set_name: Name of the data set (e.g., \"Training\", \"Validation\", \"Test\").\n",
        "    \"\"\"\n",
        "    print(f\"{set_name} Set Label Counts:\")\n",
        "    label_counts = df['labels'].value_counts()\n",
        "    print(label_counts)\n",
        "\n",
        "# Split the data into train, validation, and test sets\n",
        "train_df, valid_df, test_df = split_data(df, train_size=0.7, valid_size=0.15, test_size=0.15, random_state=0)\n",
        "\n",
        "# Print label counts for each set\n",
        "print_label_counts(train_df, \"Training\")\n",
        "print_label_counts(valid_df, \"Validation\")\n",
        "print_label_counts(test_df, \"Test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q35Yz1TvZA9R",
        "outputId": "64f85cbf-ad78-480f-abd8-fba41beaa1a6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Label Counts:\n",
            "Sick            1066\n",
            "Healthy         1044\n",
            "Tuberculosis    1040\n",
            "Name: labels, dtype: int64\n",
            "Validation Set Label Counts:\n",
            "Healthy         233\n",
            "Tuberculosis    229\n",
            "Sick            213\n",
            "Name: labels, dtype: int64\n",
            "Test Set Label Counts:\n",
            "Tuberculosis    231\n",
            "Healthy         223\n",
            "Sick            221\n",
            "Name: labels, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the input shape\n",
        "input_shape = (224, 224, 3)\n",
        "\n",
        "# Create the VGG16 base model\n",
        "vgg16 = VGG16(input_shape=input_shape, include_top=False, weights='imagenet')\n",
        "\n",
        "# Freeze the weights of the base model\n",
        "for layer in vgg16.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add SSD layers on top of the base model\n",
        "ssd_model = models.Sequential()\n",
        "ssd_model.add(vgg16)\n",
        "ssd_model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "ssd_model.add(layers.MaxPooling2D((2, 2), strides=2))\n",
        "ssd_model.add(layers.Conv2D(1024, (3, 3), activation='relu', padding='same'))\n",
        "ssd_model.add(layers.Conv2D(2048, (3, 3), activation='relu', padding='same'))\n",
        "ssd_model.add(layers.Flatten())\n",
        "ssd_model.add(layers.Dense(512, activation='relu'))\n",
        "ssd_model.add(layers.Dropout(0.5))\n",
        "ssd_model.add(layers.Dense(128, activation='relu'))\n",
        "ssd_model.add(layers.Dropout(0.5))\n",
        "ssd_model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "# Compile the SSD model\n",
        "ssd_model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define the data generators for training, validation, and test\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=10, width_shift_range=0.1, height_shift_range=0.1,\n",
        "                                   horizontal_flip=True)\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(train_df, x_col='file_paths', y_col='labels', target_size=(224, 224),\n",
        "                                                  batch_size=32)\n",
        "valid_generator = valid_datagen.flow_from_dataframe(valid_df, x_col='file_paths', y_col='labels', target_size=(224, 224),\n",
        "                                                  batch_size=32)\n",
        "test_generator = test_datagen.flow_from_dataframe(test_df, x_col='file_paths', y_col='labels', target_size=(224, 224),\n",
        "                                                 batch_size=32)\n",
        "\n",
        "# Train the SSD model\n",
        "ssd_model.fit_generator(train_generator, epochs=10, validation_data=valid_generator)\n",
        "\n",
        "# Evaluate the SSD model on the test set\n",
        "ssd_model.evaluate_generator(test_generator)\n",
        "\n",
        "# Save the SSD model\n",
        "ssd_model.save('ssd_vgg16.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BERC4rYpbIHB",
        "outputId": "73d6924d-401b-41d2-fc0a-006f15aa6184"
      },
      "execution_count": 10,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3150 validated image filenames belonging to 3 classes.\n",
            "Found 675 validated image filenames belonging to 3 classes.\n",
            "Found 675 validated image filenames belonging to 3 classes.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-e94906ad6184>:48: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  ssd_model.fit_generator(train_generator, epochs=10, validation_data=valid_generator)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "99/99 [==============================] - 3211s 32s/step - loss: 0.8731 - accuracy: 0.6143 - val_loss: 0.3266 - val_accuracy: 0.8844\n",
            "Epoch 2/10\n",
            "99/99 [==============================] - 3083s 31s/step - loss: 0.4807 - accuracy: 0.8263 - val_loss: 0.3273 - val_accuracy: 0.8815\n",
            "Epoch 3/10\n",
            "99/99 [==============================] - 3100s 31s/step - loss: 0.3546 - accuracy: 0.8711 - val_loss: 0.3751 - val_accuracy: 0.8756\n",
            "Epoch 4/10\n",
            "99/99 [==============================] - 3069s 31s/step - loss: 0.3039 - accuracy: 0.8927 - val_loss: 0.2326 - val_accuracy: 0.9111\n",
            "Epoch 5/10\n",
            "99/99 [==============================] - 3064s 31s/step - loss: 0.2931 - accuracy: 0.9003 - val_loss: 0.2101 - val_accuracy: 0.9200\n",
            "Epoch 6/10\n",
            "99/99 [==============================] - 3096s 31s/step - loss: 0.2706 - accuracy: 0.9057 - val_loss: 0.2177 - val_accuracy: 0.9230\n",
            "Epoch 7/10\n",
            "99/99 [==============================] - 3077s 31s/step - loss: 0.2490 - accuracy: 0.9121 - val_loss: 0.3259 - val_accuracy: 0.9022\n",
            "Epoch 8/10\n",
            "99/99 [==============================] - 3073s 31s/step - loss: 0.2494 - accuracy: 0.9152 - val_loss: 0.1544 - val_accuracy: 0.9437\n",
            "Epoch 9/10\n",
            "99/99 [==============================] - 3148s 32s/step - loss: 0.2002 - accuracy: 0.9263 - val_loss: 0.1777 - val_accuracy: 0.9289\n",
            "Epoch 10/10\n",
            "99/99 [==============================] - 3081s 31s/step - loss: 0.2038 - accuracy: 0.9270 - val_loss: 0.1678 - val_accuracy: 0.9422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-e94906ad6184>:51: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  ssd_model.evaluate_generator(test_generator)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ssd_model.save('ssd_model_final.h5')"
      ],
      "metadata": {
        "id": "wjrPGNSYV3p7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score, recall_score\n",
        "\n",
        "# Assuming you have test data and labels\n",
        "test_data, test_labels = next(valid_generator)\n",
        "\n",
        "# Predictions from the model\n",
        "predictions = ssd_model.predict(test_data)\n",
        "\n",
        "# Convert one-hot encoded labels to single labels\n",
        "true_labels = np.argmax(test_labels, axis=1)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "# AUC\n",
        "# AUC is meaningful for binary classification problems, so you might need to adjust this part based on your specific use case.\n",
        "# For simplicity, let's assume you are evaluating a binary classification task.\n",
        "# You can use roc_auc_score for multi-class problems with one-hot encoded labels as well.\n",
        "auc = roc_auc_score(test_labels[:, 1], predictions[:, 1])\n",
        "print(f'AUC: {auc}')\n",
        "\n",
        "# Average Precision\n",
        "# You need to calculate average precision separately for each class and then take the average\n",
        "average_precision = average_precision_score(test_labels, predictions, average='micro')\n",
        "print(f'Ave. Prec.: {average_precision}')\n",
        "\n",
        "# Average Recall\n",
        "# You need to calculate recall separately for each class and then take the average\n",
        "average_recall = recall_score(true_labels, predicted_labels, average='macro')\n",
        "print(f'Ave. Rec.: {average_recall}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g99EyOelWIcS",
        "outputId": "18d0fd4a-2d3e-48d1-a7d8-b52b25c31383"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 33s 33s/step\n",
            "Accuracy: 0.90625\n",
            "AUC: 0.9708333333333333\n",
            "Ave. Prec.: 0.9706805506414882\n",
            "Ave. Rec.: 0.9048821548821548\n"
          ]
        }
      ]
    }
  ]
}