{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cWDueGryoZKD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required library\n",
        "!pip install gdown\n",
        "\n",
        "# Provide the file ID\n",
        "file_id = '1r-oNYTPiPCOUzSjChjCIYTdkjBTugqxR'\n",
        "\n",
        "# Create a download link\n",
        "download_link = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "# Download the file\n",
        "!gdown $download_link\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sF__HtZo518",
        "outputId": "5251224c-0bcf-4250-a664-b777784b6bf6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.11.17)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1r-oNYTPiPCOUzSjChjCIYTdkjBTugqxR\n",
            "From (redirected): https://drive.google.com/uc?id=1r-oNYTPiPCOUzSjChjCIYTdkjBTugqxR&confirm=t&uuid=dbc10697-eff6-4fd0-b717-db2eea68e853\n",
            "To: /content/TBX11K.zip\n",
            "100% 3.31G/3.31G [00:49<00:00, 67.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Install the required library\n",
        "!pip install zipfile\n",
        "\n",
        "# Specify the name of the downloaded ZIP file\n",
        "zip_file_path = '/content/TBX11K.zip'\n",
        "\n",
        "# Specify the extraction directory\n",
        "extracted_path = '/folders/1LqXQFENogsBlh9N-z9jFaid0qlgNzEVE'\n",
        "\n",
        "# Extract the contents of the ZIP file\n",
        "import zipfile\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_path)\n",
        "\"\"\"\n",
        "# Specify the name of the downloaded ZIP file\n",
        "zip_file_path = '/content/TBX11K.zip'\n",
        "\n",
        "# Specify the extraction directory\n",
        "extracted_path = '/content/Dataset'\n",
        "\n",
        "# Extract the contents of the ZIP file\n",
        "import zipfile\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_path)\n"
      ],
      "metadata": {
        "id": "xG6reAwqo7wG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dirlist = ['/content/Dataset/TBX11K/imgs/health', '/content/Dataset/TBX11K/imgs/sick', '/content/Dataset/TBX11K/imgs/tb']\n",
        "classes = ['Healthy', 'Sick', 'Tuberculosis']\n",
        "filepaths = []\n",
        "labels = []\n",
        "for d, c in zip(dirlist, classes):\n",
        "    flist = os.listdir(d)\n",
        "    for f in flist:\n",
        "        fpath = os.path.join(d, f)\n",
        "        filepaths.append(fpath)\n",
        "        labels.append(c)\n",
        "print ('filepaths: ', len(filepaths), '   labels: ', len(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl7Y0baGo-Nd",
        "outputId": "5b229e9a-6154-4447-fcf4-94fb6049b9d9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "filepaths:  8400    labels:  8400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Fseries = pd.Series(filepaths, name='file_paths')\n",
        "Lseries = pd.Series(labels, name='labels')\n",
        "\n",
        "# Ensure lengths match\n",
        "min_length = min(len(Fseries), len(Lseries))\n",
        "Fseries = Fseries[:min_length]\n",
        "Lseries = Lseries[:min_length]\n",
        "\n",
        "# Create the DataFrame with named columns\n",
        "df = pd.concat([Fseries, Lseries], axis=1)\n",
        "df.columns = ['file_paths', 'labels']\n",
        "\n",
        "# Count occurrences of each label\n",
        "label_counts = df['labels'].value_counts()\n",
        "print(label_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwanu2KGo_6Q",
        "outputId": "d81f81af-9496-4736-fa28-bd67ef9b8d48"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Healthy         3800\n",
            "Sick            3800\n",
            "Tuberculosis     800\n",
            "Name: labels, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_count = 1500\n",
        "samples = []\n",
        "\n",
        "for category in df['labels'].unique():\n",
        "    category_slice = df.query(\"labels == @category\")\n",
        "\n",
        "    if len(category_slice) < file_count:\n",
        "        # If the number of files in the category is less than file_count,\n",
        "        # sample with replacement to fill up the required number of samples\n",
        "        samples.append(category_slice.sample(file_count, replace=True, random_state=1))\n",
        "    else:\n",
        "        samples.append(category_slice.sample(file_count, replace=False, random_state=1))\n",
        "\n",
        "df = pd.concat(samples, axis=0).sample(frac=1.0, random_state=1).reset_index(drop=True)\n",
        "print(df['labels'].value_counts())\n",
        "print(len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qwx0v5JpBU3",
        "outputId": "11c8d706-6740-4233-ade3-458fd05a1995"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sick            1500\n",
            "Healthy         1500\n",
            "Tuberculosis    1500\n",
            "Name: labels, dtype: int64\n",
            "4500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(df, train_size=0.7, valid_size=0.15, test_size=0.15, random_state=None):\n",
        "    \"\"\"\n",
        "    Split the data into training, validation, and test sets.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame containing the data to be split.\n",
        "    - train_size: The proportion of data to include in the training set (default: 0.7).\n",
        "    - valid_size: The proportion of data to include in the validation set (default: 0.15).\n",
        "    - test_size: The proportion of data to include in the test set (default: 0.15).\n",
        "    - random_state: Seed for random number generation (optional).\n",
        "\n",
        "    Returns:\n",
        "    - train_df: DataFrame for training.\n",
        "    - valid_df: DataFrame for validation.\n",
        "    - test_df: DataFrame for testing.\n",
        "    \"\"\"\n",
        "    if train_size + valid_size + test_size != 1.0:\n",
        "        raise ValueError(\"The sum of train_size, valid_size, and test_size should be 1.0\")\n",
        "\n",
        "    # Split the data into training and test sets\n",
        "    train_and_valid_df, test_df = train_test_split(df, test_size=test_size, random_state=random_state)\n",
        "\n",
        "    # Further split the training and validation data\n",
        "    train_df, valid_df = train_test_split(train_and_valid_df,\n",
        "                                          train_size=train_size / (train_size + valid_size),\n",
        "                                          random_state=random_state)\n",
        "\n",
        "    return train_df, valid_df, test_df\n",
        "\n",
        "def print_label_counts(df, set_name):\n",
        "    \"\"\"\n",
        "    Print label counts for a given DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame for which label counts should be printed.\n",
        "    - set_name: Name of the data set (e.g., \"Training\", \"Validation\", \"Test\").\n",
        "    \"\"\"\n",
        "    print(f\"{set_name} Set Label Counts:\")\n",
        "    label_counts = df['labels'].value_counts()\n",
        "    print(label_counts)\n",
        "\n",
        "# Split the data into train, validation, and test sets\n",
        "train_df, valid_df, test_df = split_data(df, train_size=0.7, valid_size=0.15, test_size=0.15, random_state=0)\n",
        "\n",
        "# Print label counts for each set\n",
        "print_label_counts(train_df, \"Training\")\n",
        "print_label_counts(valid_df, \"Validation\")\n",
        "print_label_counts(test_df, \"Test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7fEzniApC5M",
        "outputId": "2188a9f4-e536-4ab4-b3f2-1f1fa3844d45"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Label Counts:\n",
            "Sick            1066\n",
            "Healthy         1044\n",
            "Tuberculosis    1040\n",
            "Name: labels, dtype: int64\n",
            "Validation Set Label Counts:\n",
            "Healthy         233\n",
            "Tuberculosis    229\n",
            "Sick            213\n",
            "Name: labels, dtype: int64\n",
            "Test Set Label Counts:\n",
            "Tuberculosis    231\n",
            "Healthy         223\n",
            "Sick            221\n",
            "Name: labels, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import VGG16  # Import VGG16\n",
        "\n",
        "# Define the backbone network (VGG16)\n",
        "base_model = VGG16(\n",
        "    include_top=False,\n",
        "    input_shape=(224, 224, 3),\n",
        "    weights='imagenet'\n",
        ")\n",
        "\n",
        "# Define the region proposal network (RPN)\n",
        "# This is a simplified version for demonstration purposes\n",
        "# In practice, you would use a pre-trained RPN or custom implementation\n",
        "rpn_input = base_model.output\n",
        "rpn_output = Conv2D(256, (3, 3), activation='relu')(rpn_input)\n",
        "rpn_output = MaxPooling2D(pool_size=(2, 2))(rpn_output)\n",
        "rpn_output = Flatten()(rpn_output)\n",
        "rpn_output = Dense(256, activation='relu')(rpn_output)\n",
        "\n",
        "# Create the Fast R-CNN model\n",
        "fast_rcnn_input = Input(shape=(224, 224, 3))  # Specify the input shape\n",
        "roi_pooling_layer = Model(inputs=base_model.input, outputs=base_model.get_layer('block5_pool').output)  # Use the correct layer name for VGG16\n",
        "roi_pooled_features = roi_pooling_layer(fast_rcnn_input)\n",
        "\n",
        "fast_rcnn_output = GlobalAveragePooling2D()(roi_pooled_features)\n",
        "fast_rcnn_output = Dense(128, activation='relu')(fast_rcnn_output)\n",
        "fast_rcnn_output = tf.keras.layers.BatchNormalization()(fast_rcnn_output)\n",
        "fast_rcnn_output = tf.keras.layers.Dropout(0.2)(fast_rcnn_output)\n",
        "fast_rcnn_output = Dense(3, activation='softmax')(fast_rcnn_output)\n",
        "\n",
        "fast_rcnn_model = Model(inputs=fast_rcnn_input, outputs=fast_rcnn_output)\n",
        "\n",
        "# Optional: You may want to freeze the weights of the backbone network\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the Fast R-CNN model with an appropriate loss and optimizer\n",
        "fast_rcnn_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Print a summary of the Fast R-CNN model\n",
        "fast_rcnn_model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDpNmOgQpE7f",
        "outputId": "98b25c46-3868-42bb-9935-d8fd5272abbc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 2s 0us/step\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " model (Functional)          (None, 7, 7, 512)         14714688  \n",
            "                                                                 \n",
            " global_average_pooling2d (  (None, 512)               0         \n",
            " GlobalAveragePooling2D)                                         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               65664     \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 128)               512       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14781251 (56.39 MB)\n",
            "Trainable params: 66307 (259.01 KB)\n",
            "Non-trainable params: 14714944 (56.13 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_size = (224, 224)\n",
        "batch_size = 4\n",
        "\n",
        "train_datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.efficientnet.preprocess_input, horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.efficientnet.preprocess_input)\n",
        "train_gen = train_datagen.flow_from_dataframe(train_df, x_col='file_paths', y_col='labels', target_size=target_size, batch_size=batch_size, color_mode='rgb', class_mode='categorical')\n",
        "valid_gen = test_datagen.flow_from_dataframe(valid_df, x_col='file_paths', y_col='labels', target_size=target_size, batch_size=batch_size, color_mode='rgb', class_mode='categorical')\n",
        "test_gen = test_datagen.flow_from_dataframe(test_df, x_col='file_paths', y_col='labels', target_size=target_size, batch_size=batch_size, color_mode='rgb', class_mode='categorical')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Y2hCFSvrLfz",
        "outputId": "aa6d71d5-2c47-4a1f-9695-3a4b0c365bed"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3150 validated image filenames belonging to 3 classes.\n",
            "Found 675 validated image filenames belonging to 3 classes.\n",
            "Found 675 validated image filenames belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_steps_per_epoch = len(train_gen)\n",
        "valid_steps_per_epoch = len(valid_gen)\n",
        "\n",
        "# Define the number of epochs\n",
        "num_epochs = 20  # You can adjust this as needed\n",
        "\n",
        "# Fit the Fast R-CNN model\n",
        "history = fast_rcnn_model.fit_generator(\n",
        "    train_gen,\n",
        "    steps_per_epoch=train_steps_per_epoch,\n",
        "    epochs=num_epochs,\n",
        "    validation_data=valid_gen,\n",
        "    validation_steps=valid_steps_per_epoch\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_results = fast_rcnn_model.evaluate(test_gen)\n",
        "\n",
        "# Print test results (e.g., test loss and test accuracy)\n",
        "print(\"Test Loss:\", test_results[0])\n",
        "print(\"Test Accuracy:\", test_results[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzRJmGdbq1qv",
        "outputId": "64832631-658e-41e6-b11d-36c10df95ad6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-7fcd6d867375>:8: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = fast_rcnn_model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "788/788 [==============================] - 49s 50ms/step - loss: 0.5415 - accuracy: 0.7854 - val_loss: 0.3268 - val_accuracy: 0.8859\n",
            "Epoch 2/20\n",
            "788/788 [==============================] - 37s 46ms/step - loss: 0.4458 - accuracy: 0.8244 - val_loss: 0.2648 - val_accuracy: 0.9081\n",
            "Epoch 3/20\n",
            "788/788 [==============================] - 36s 45ms/step - loss: 0.3951 - accuracy: 0.8422 - val_loss: 0.2608 - val_accuracy: 0.9111\n",
            "Epoch 4/20\n",
            "788/788 [==============================] - 35s 45ms/step - loss: 0.4090 - accuracy: 0.8451 - val_loss: 0.2004 - val_accuracy: 0.9363\n",
            "Epoch 5/20\n",
            "788/788 [==============================] - 35s 45ms/step - loss: 0.4264 - accuracy: 0.8406 - val_loss: 0.1834 - val_accuracy: 0.9437\n",
            "Epoch 6/20\n",
            "788/788 [==============================] - 36s 46ms/step - loss: 0.3823 - accuracy: 0.8587 - val_loss: 0.2247 - val_accuracy: 0.9200\n",
            "Epoch 7/20\n",
            "788/788 [==============================] - 35s 45ms/step - loss: 0.4023 - accuracy: 0.8514 - val_loss: 0.1783 - val_accuracy: 0.9437\n",
            "Epoch 8/20\n",
            "788/788 [==============================] - 40s 50ms/step - loss: 0.3704 - accuracy: 0.8584 - val_loss: 0.2022 - val_accuracy: 0.9378\n",
            "Epoch 9/20\n",
            "788/788 [==============================] - 36s 46ms/step - loss: 0.3773 - accuracy: 0.8565 - val_loss: 0.2185 - val_accuracy: 0.9319\n",
            "Epoch 10/20\n",
            "788/788 [==============================] - 35s 45ms/step - loss: 0.3814 - accuracy: 0.8578 - val_loss: 0.2241 - val_accuracy: 0.9452\n",
            "Epoch 11/20\n",
            "788/788 [==============================] - 35s 45ms/step - loss: 0.3903 - accuracy: 0.8527 - val_loss: 0.1897 - val_accuracy: 0.9615\n",
            "Epoch 12/20\n",
            "788/788 [==============================] - 36s 45ms/step - loss: 0.3255 - accuracy: 0.8794 - val_loss: 0.2885 - val_accuracy: 0.9081\n",
            "Epoch 13/20\n",
            "788/788 [==============================] - 40s 51ms/step - loss: 0.3762 - accuracy: 0.8616 - val_loss: 0.1897 - val_accuracy: 0.9541\n",
            "Epoch 14/20\n",
            "788/788 [==============================] - 35s 45ms/step - loss: 0.3744 - accuracy: 0.8616 - val_loss: 0.3079 - val_accuracy: 0.9052\n",
            "Epoch 15/20\n",
            "788/788 [==============================] - 36s 46ms/step - loss: 0.3758 - accuracy: 0.8625 - val_loss: 0.2298 - val_accuracy: 0.9452\n",
            "Epoch 16/20\n",
            "788/788 [==============================] - 37s 47ms/step - loss: 0.3490 - accuracy: 0.8616 - val_loss: 0.2300 - val_accuracy: 0.9363\n",
            "Epoch 17/20\n",
            "788/788 [==============================] - 41s 51ms/step - loss: 0.3785 - accuracy: 0.8568 - val_loss: 0.1586 - val_accuracy: 0.9541\n",
            "Epoch 18/20\n",
            "788/788 [==============================] - 39s 49ms/step - loss: 0.3690 - accuracy: 0.8616 - val_loss: 0.2259 - val_accuracy: 0.9319\n",
            "Epoch 19/20\n",
            "788/788 [==============================] - 38s 48ms/step - loss: 0.3607 - accuracy: 0.8673 - val_loss: 0.1959 - val_accuracy: 0.9393\n",
            "Epoch 20/20\n",
            "788/788 [==============================] - 37s 47ms/step - loss: 0.3699 - accuracy: 0.8638 - val_loss: 0.1522 - val_accuracy: 0.9541\n",
            "169/169 [==============================] - 7s 40ms/step - loss: 0.1872 - accuracy: 0.9363\n",
            "Test Loss: 0.18719060719013214\n",
            "Test Accuracy: 0.936296284198761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score, recall_score\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Assuming fast_rcnn_model is already defined and compiled\n",
        "\n",
        "# Generate predictions for the test set\n",
        "y_pred_prob = fast_rcnn_model.predict(test_gen)\n",
        "y_true = to_categorical(test_gen.classes)  # Convert to one-hot encoding\n",
        "\n",
        "# Calculate accuracy\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "accuracy = accuracy_score(np.argmax(y_true, axis=1), y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate AUC\n",
        "auc = roc_auc_score(y_true, y_pred_prob, multi_class='ovr', average='weighted')\n",
        "print(\"AUC:\", auc)\n",
        "\n",
        "# Calculate average precision\n",
        "average_precision = average_precision_score(y_true, y_pred_prob, average='weighted')\n",
        "print(\"Average Precision:\", average_precision)\n",
        "\n",
        "# Calculate average recall\n",
        "average_recall = recall_score(np.argmax(y_true, axis=1), y_pred, average='weighted')\n",
        "print(\"Average Recall:\", average_recall)\n",
        "\n",
        "# Evaluate the model on the test set using the Keras evaluate method\n",
        "test_results = fast_rcnn_model.evaluate(test_gen)\n",
        "\n",
        "# Print test results (e.g., test loss and test accuracy)\n",
        "print(\"Test Loss:\", test_results[0])\n",
        "print(\"Test Accuracy:\", test_results[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34QI7mkxvFkc",
        "outputId": "99c67115-5159-4717-9cbb-521cfaac101c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "169/169 [==============================] - 6s 36ms/step\n",
            "Accuracy: 0.3362962962962963\n",
            "AUC: 0.508706382236329\n",
            "Average Precision: 0.3471450962031932\n",
            "Average Recall: 0.3362962962962963\n",
            "169/169 [==============================] - 7s 40ms/step - loss: 0.1872 - accuracy: 0.9363\n",
            "Test Loss: 0.18719060719013214\n",
            "Test Accuracy: 0.936296284198761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Test Loss:\", test_results[0])\n",
        "print(\"Test Accuracy:\", test_results[1])\n",
        "\n",
        "# Generate predictions for the test set\n",
        "y_pred_prob = fast_rcnn_model.predict(test_gen)\n",
        "y_true = to_categorical(test_gen.classes)  # Convert to one-hot encoding\n",
        "\n",
        "# Calculate AUC\n",
        "auc = roc_auc_score(y_true, y_pred_prob, multi_class='ovr', average='weighted')\n",
        "print(\"AUC:\", auc)\n",
        "\n",
        "# Calculate average precision\n",
        "average_precision = average_precision_score(y_true, y_pred_prob, average='weighted')\n",
        "print(\"Average Precision:\", average_precision)\n",
        "\n",
        "# Calculate average recall\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "average_recall = recall_score(np.argmax(y_true, axis=1), y_pred, average='weighted')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biWxdUJZw_cc",
        "outputId": "55eb9d1a-b708-4956-a3e1-60052ea6a224"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.18719060719013214\n",
            "Test Accuracy: 0.936296284198761\n",
            "169/169 [==============================] - 8s 47ms/step\n",
            "AUC: 0.47768736131814893\n",
            "Average Precision: 0.3259831710336418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fast_rcnn_model.save(\"fast_rcnn_model.h5\")"
      ],
      "metadata": {
        "id": "999EzYK2xLYH",
        "outputId": "e08ebd5f-9e52-40c2-da1a-367609ddbb62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    }
  ]
}